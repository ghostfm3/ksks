{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classCBOW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGBK26OyilOnDbjRNoB+Vv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghostfm3/ksks/blob/master/classCBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYRUL1brd1bk"
      },
      "source": [
        "class SimpleCBOW:\r\n",
        "  #イニシャライザの引数として、語彙数のvocab_sizeと中間層のニューロン数のhidden_sizeを取る\r\n",
        "  def __init__(self, vocab_size, hidden_size):\r\n",
        "    V, H = vocab_size, hidden_size\r\n",
        "  #重みの初期化について、重みW_inとW_outを作成する。2つの重みはそれぞれ小さなランダム値によって初期化する。またこの時NumPy配列のデータ型をastype('f')で指定する→これにより32bitの浮動小数点数で初期化される\r\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype('f')\r\n",
        "    W_out = 0.01 * np.random.randn(H, V).astype('f')\r\n",
        "  #入力側のMatNulレイヤーを2つ、出力側のMatNulレイヤーを1つ、softmaxlossレイヤを1つ生成する\r\n",
        "    self.in_layer0 = MatMul(W_in)\r\n",
        "    self.in_layer1 = MatMul(W_in)\r\n",
        "    self.out_layer = MatMul(W_out)\r\n",
        "    self.loss_layer = SoftmaxWithLoss()\r\n",
        "  #すべての重みと勾配をリストにまとめる\r\n",
        "    layers = [seif.in_layer0, self.in_layer1, self.out_layer]\r\n",
        "    self.params, self.grads = {}, []\r\n",
        "    for layer in layers:\r\n",
        "      self.params += layer.params\r\n",
        "      self.grads += layer.grads\r\n",
        "   #メンバ変数に単語の分散表現を設定 \r\n",
        "    self.word_vecs = W_in\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdyTNgmAexzL"
      },
      "source": [
        "↑simpleCBOWクラスのイニシャライザ\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LFggI-8d9RV"
      },
      "source": [
        "class SimpleCBOW:\r\n",
        "  def __init__(self, vocab_size, hidden_size):\r\n",
        "    V, H = vocab_size, hidden_size\r\n",
        "\r\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype('f')\r\n",
        "    W_out = 0.01 * np.random.randn(H, V).astype('f')\r\n",
        "\r\n",
        "    self.in_layer0 = MatMul(W_in)\r\n",
        "    self.in_layer1 = MatMul(W_in)\r\n",
        "    self.out_layer = MatMul(W_out)\r\n",
        "    self.loss_layer = SoftmaxWithLoss()\r\n",
        "\r\n",
        "    layers = [seif.in_layer0, self.in_layer1, self.out_layer]\r\n",
        "    self.params, self.grads = {}, []\r\n",
        "    for layer in layers:\r\n",
        "      self.params += layer.params\r\n",
        "      self.grads += layer.grads\r\n",
        "    \r\n",
        "    self.word_vecs = W_in\r\n",
        "\r\n",
        "  def forward(self, contexts, target):\r\n",
        "    h0 = self.in_layer0.forward(contexts[:, 0])\r\n",
        "    h1 = self.in_layers1.forward(contexts[:, 1])\r\n",
        "    h = (h0 + h1) * 0.5\r\n",
        "    score = self.out_layer.forward(h)\r\n",
        "    loss = self.loss_layer.forward(score, target)\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np1YHXvHey85"
      },
      "source": [
        "↑ニューラルネットワークの順伝播であるforward()メソッド"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chVnQc4Eewm3"
      },
      "source": [
        "class SimpleCBOW:\r\n",
        "  def __init__(self, vocab_size, hidden_size):\r\n",
        "    V, H = vocab_size, hidden_size\r\n",
        "\r\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype('f')\r\n",
        "    W_out = 0.01 * np.random.randn(H, V).astype('f')\r\n",
        "\r\n",
        "    self.in_layer0 = MatMul(W_in)\r\n",
        "    self.in_layer1 = MatMul(W_in)\r\n",
        "    self.out_layer = MatMul(W_out)\r\n",
        "    self.loss_layer = SoftmaxWithLoss()\r\n",
        "\r\n",
        "    layers = [seif.in_layer0, self.in_layer1, self.out_layer]\r\n",
        "    self.params, self.grads = {}, []\r\n",
        "    for layer in layers:\r\n",
        "      self.params += layer.params\r\n",
        "      self.grads += layer.grads\r\n",
        "    \r\n",
        "    self.word_vecs = W_in\r\n",
        "\r\n",
        "  def forward(self, contexts, target):\r\n",
        "    h0 = self.in_layer0.forward(contexts[:, 0])\r\n",
        "    h1 = self.in_layers1.forward(contexts[:, 1])\r\n",
        "    h = (h0 + h1) * 0.5\r\n",
        "    score = self.out_layer.forward(h)\r\n",
        "    loss = self.loss_layer.forward(score, target)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def backward(self, dout=1):\r\n",
        "   ds = self.loss_layer.backward(dout)\r\n",
        "   da = self.out_layer.backward(ds)\r\n",
        "   da *= 0.5\r\n",
        "   self.in_layer1.backward(da)\r\n",
        "   self.in_layer0.backward(da)\r\n",
        "   return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKxAkY28ezuW"
      },
      "source": [
        "↑逆伝播のbackward()メソッド"
      ]
    }
  ]
}