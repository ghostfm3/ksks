{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "畳み込み",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "71K9_Py8a3lE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6bdbd1bd-efb6-4ed1-b154-89a6749639e6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "print (sys.version)\n",
        "print (\"Tensorflow\", tf.__version__)\n",
        "print (\"Keras\", keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.9 (default, Jul 17 2020, 12:50:27) \n",
            "[GCC 8.4.0]\n",
            "Tensorflow 2.3.0\n",
            "Keras 2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76P2P3NMbQZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# proxy 環境下だと以下 mnist.load_data() でエラー吐く。\n",
        "# これは内部的に mnist データを urllib ライブラリをつかってダウンロードしているため。\n",
        "# proxy 設定を変更する\n",
        "\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac_lq_N4bhIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73d74a41-c495-49f0-ee90-14fb4ac9bb5e"
      },
      "source": [
        "print(type(x_train), type(y_train))\n",
        "print(type(x_test), type(y_test))\n",
        "\n",
        "print(x_train.shape, y_train.shape) # 6万個の訓練データ：画像(28ピクセル×28ピクセル), ラベル\n",
        "print(x_test.shape, y_test.shape)   # 1万個のテストデータ: 画像(28ピクセル×28ピクセル), ラベル\n",
        "\n",
        "# 画像とラベルの組み合わせを1セット確認\n",
        "print (x_train[0])\n",
        "print (y_train[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY713IuGcGcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)  #  28 x 28 の画像データを 784 ベクトルとして扱う。60000 x 784\n",
        "x_train = x_train.astype('float32')    # 入力を実数値として扱うため int型からfloat型に変換\n",
        "x_train = x_train / 255                #  255で割り、0〜1の実数値に変換  \n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)  # 0-9の整数値を1-of-K符号化法に変更\n",
        "\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test = x_test / 255\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uh-6QZVcHc5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc54cbd0-1ad7-45bd-c1d1-f94aa0b340c1"
      },
      "source": [
        "print (x_train[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
            " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215687\n",
            " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
            " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
            " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313726\n",
            " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.13725491 0.94509804\n",
            " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
            " 0.5882353  0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
            " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.5803922\n",
            " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058825\n",
            " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
            " 0.3137255  0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333336 0.99215686\n",
            " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrFvL9LDcKlp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "376f1874-8a93-4c07-fc46-69714499f5a0"
      },
      "source": [
        "print (y_train[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5_Me_0Ged3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXAqGcEcuRkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dropout, Conv2D, MaxPooling2D, Flatten"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEqqKhwEduGL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "154e1da9-4c57-49a3-d513-945fb97d267f"
      },
      "source": [
        "#kernel_size=\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "x_train = x_train.reshape(60000, 28, 28, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "\n",
        "x_test = x_test.reshape(10000, 28, 28, 1)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3, 3), input_shape=(28, 28, 1), activation='relu',))\n",
        "#model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, Activation('softmax')))\n",
        "model.add(Dense(units=200, input_dim=784)) # Dense=全結合層\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(units=10))\n",
        "model.add(Activation('softmax')) # 活性化関数\n",
        "\n",
        "# 訓練プロセスの作成: 勾配法=SGD、損失関数=binary_crossentropy、評価関数：-）\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1))\n",
        "# モデルの概要を表示\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                108170    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               2200      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2010      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 112,540\n",
            "Trainable params: 112,540\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx3i2J97d2xA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "outputId": "c0baa6cb-0846-405c-81a1-ccde3ce4d058"
      },
      "source": [
        "l = model.layers[0]\n",
        "print (l.get_weights())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[[[-0.18257271,  0.036965  , -0.03348345, -0.12601611,\n",
            "          -0.10622332, -0.04729654,  0.11985539,  0.1922201 ,\n",
            "          -0.10572692,  0.10900103,  0.16628684, -0.13725217,\n",
            "          -0.09381367,  0.13818328, -0.02115779,  0.04204494]],\n",
            "\n",
            "        [[ 0.09499915,  0.13549425,  0.04365034,  0.04977064,\n",
            "          -0.11386844,  0.05152029, -0.15575233, -0.02716467,\n",
            "           0.19725604, -0.02185051,  0.09326838,  0.04780295,\n",
            "          -0.10577677, -0.15946202,  0.09877898,  0.0744967 ]],\n",
            "\n",
            "        [[-0.10904705,  0.04885809,  0.14712663,  0.03785568,\n",
            "          -0.06713195,  0.10986568,  0.17764731, -0.10794366,\n",
            "           0.08688842,  0.14469479,  0.15623946, -0.06726415,\n",
            "           0.13782851, -0.09150255,  0.12517731, -0.11091979]]],\n",
            "\n",
            "\n",
            "       [[[-0.0783038 , -0.14292538,  0.12788092, -0.16594473,\n",
            "           0.168337  , -0.13106412, -0.12405913, -0.11724131,\n",
            "           0.16299953, -0.12729901,  0.1578599 , -0.07971219,\n",
            "           0.08934416, -0.01504537,  0.17555313, -0.10132195]],\n",
            "\n",
            "        [[ 0.05471121, -0.0952968 , -0.12180259, -0.14720637,\n",
            "           0.14969365, -0.07099392, -0.14162682,  0.17833553,\n",
            "          -0.13084084, -0.03481501,  0.01864354, -0.12810688,\n",
            "          -0.1964636 ,  0.16598718,  0.17950438,  0.04101968]],\n",
            "\n",
            "        [[ 0.0706187 , -0.15842997, -0.16301355,  0.1655715 ,\n",
            "          -0.15327114,  0.09543364,  0.15732907, -0.06385744,\n",
            "           0.06452616, -0.13091384,  0.1057017 , -0.15540034,\n",
            "          -0.18958804,  0.0054431 , -0.1874271 ,  0.11255853]]],\n",
            "\n",
            "\n",
            "       [[[ 0.12172572, -0.10174669, -0.08418491,  0.06033953,\n",
            "          -0.17913318, -0.0705324 ,  0.11631255,  0.07210885,\n",
            "           0.03549796, -0.08741882, -0.15187284, -0.12771103,\n",
            "          -0.18586442,  0.10120006, -0.16577838,  0.09688611]],\n",
            "\n",
            "        [[ 0.07613625, -0.0957907 , -0.1373582 ,  0.08170165,\n",
            "           0.09401117,  0.00384425, -0.15165685,  0.1876914 ,\n",
            "          -0.05270848,  0.08485837, -0.019379  , -0.03463928,\n",
            "           0.10965423,  0.13500686,  0.12535654,  0.01594514]],\n",
            "\n",
            "        [[ 0.17712729, -0.16500753, -0.04273453,  0.00853226,\n",
            "          -0.00820677,  0.17313643,  0.14592801,  0.12102853,\n",
            "          -0.02143002,  0.01973654,  0.00457096,  0.04404005,\n",
            "           0.17854299, -0.08636194,  0.16309918, -0.10535969]]]],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8EBG0KCd5eO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc2c037c-6e98-4e74-dca1-0996c4e510bf"
      },
      "source": [
        "# 第1引数、第2引数: 入力データ\n",
        "# batch_size: バッチサイズ。\n",
        "# epochs: 訓練を行う回数を指定します。\n",
        "# verbose: 指定する数値により実行ログの出す内容が変わります。\n",
        "# validation_data: 評価用のデータを指定します。\n",
        "# 戻り値・・・訓練の損失値と評価関数値が返ってきます。\n",
        "model.compile(loss='categorical_crossentropy',optimizer=SGD(lr=0.1),metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=100, batch_size=1000, validation_data=(x_test, y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 1/60 [..............................] - ETA: 0s - loss: 1.5460 - accuracy: 0.3140WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0059s vs `on_train_batch_end` time: 0.0142s). Check your callbacks.\n",
            "58/60 [============================>.] - ETA: 0s - loss: 1.5772 - accuracy: 0.3055WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_test_batch_end` time: 0.0047s). Check your callbacks.\n",
            "60/60 [==============================] - 1s 21ms/step - loss: 1.5764 - accuracy: 0.3053 - val_loss: 1.5491 - val_accuracy: 0.3103\n",
            "Epoch 2/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5427 - accuracy: 0.3101 - val_loss: 1.5357 - val_accuracy: 0.3074\n",
            "Epoch 3/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5357 - accuracy: 0.3119 - val_loss: 1.5297 - val_accuracy: 0.3037\n",
            "Epoch 4/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5279 - accuracy: 0.3115 - val_loss: 1.5218 - val_accuracy: 0.3107\n",
            "Epoch 5/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5262 - accuracy: 0.3079 - val_loss: 1.5398 - val_accuracy: 0.3056\n",
            "Epoch 6/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5144 - accuracy: 0.3107 - val_loss: 1.5125 - val_accuracy: 0.3097\n",
            "Epoch 7/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5186 - accuracy: 0.3131 - val_loss: 1.7018 - val_accuracy: 0.2793\n",
            "Epoch 8/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5182 - accuracy: 0.3139 - val_loss: 1.5046 - val_accuracy: 0.3066\n",
            "Epoch 9/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5018 - accuracy: 0.3164 - val_loss: 1.4987 - val_accuracy: 0.3138\n",
            "Epoch 10/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.5004 - accuracy: 0.3196 - val_loss: 1.5253 - val_accuracy: 0.3065\n",
            "Epoch 11/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4944 - accuracy: 0.3180 - val_loss: 1.5035 - val_accuracy: 0.3162\n",
            "Epoch 12/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4910 - accuracy: 0.3267 - val_loss: 1.4892 - val_accuracy: 0.3156\n",
            "Epoch 13/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4864 - accuracy: 0.3227 - val_loss: 1.4850 - val_accuracy: 0.3178\n",
            "Epoch 14/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4808 - accuracy: 0.3285 - val_loss: 1.4815 - val_accuracy: 0.3195\n",
            "Epoch 15/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4802 - accuracy: 0.3308 - val_loss: 1.4790 - val_accuracy: 0.3242\n",
            "Epoch 16/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4717 - accuracy: 0.3359 - val_loss: 1.4695 - val_accuracy: 0.3236\n",
            "Epoch 17/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4707 - accuracy: 0.3410 - val_loss: 1.4773 - val_accuracy: 0.3237\n",
            "Epoch 18/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4687 - accuracy: 0.3447 - val_loss: 1.4612 - val_accuracy: 0.3745\n",
            "Epoch 19/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4543 - accuracy: 0.3570 - val_loss: 1.4528 - val_accuracy: 0.3602\n",
            "Epoch 20/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4502 - accuracy: 0.3600 - val_loss: 1.4538 - val_accuracy: 0.3665\n",
            "Epoch 21/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4411 - accuracy: 0.3704 - val_loss: 1.4490 - val_accuracy: 0.3843\n",
            "Epoch 22/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4427 - accuracy: 0.3765 - val_loss: 1.4338 - val_accuracy: 0.3636\n",
            "Epoch 23/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 1.4308 - accuracy: 0.3865 - val_loss: 1.4312 - val_accuracy: 0.4026\n",
            "Epoch 24/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4167 - accuracy: 0.3969 - val_loss: 1.4095 - val_accuracy: 0.4007\n",
            "Epoch 25/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4135 - accuracy: 0.4031 - val_loss: 1.4074 - val_accuracy: 0.4105\n",
            "Epoch 26/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.4029 - accuracy: 0.4058 - val_loss: 1.4036 - val_accuracy: 0.4032\n",
            "Epoch 27/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3959 - accuracy: 0.4149 - val_loss: 1.3890 - val_accuracy: 0.4278\n",
            "Epoch 28/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3841 - accuracy: 0.4252 - val_loss: 1.3773 - val_accuracy: 0.4258\n",
            "Epoch 29/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3741 - accuracy: 0.4367 - val_loss: 1.3697 - val_accuracy: 0.4464\n",
            "Epoch 30/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3675 - accuracy: 0.4390 - val_loss: 1.3585 - val_accuracy: 0.4496\n",
            "Epoch 31/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3547 - accuracy: 0.4537 - val_loss: 1.3477 - val_accuracy: 0.4682\n",
            "Epoch 32/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3374 - accuracy: 0.4606 - val_loss: 1.3528 - val_accuracy: 0.4382\n",
            "Epoch 33/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3226 - accuracy: 0.4709 - val_loss: 1.3139 - val_accuracy: 0.4687\n",
            "Epoch 34/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.3026 - accuracy: 0.4774 - val_loss: 1.3159 - val_accuracy: 0.4399\n",
            "Epoch 35/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.2845 - accuracy: 0.4890 - val_loss: 1.2895 - val_accuracy: 0.5187\n",
            "Epoch 36/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.2716 - accuracy: 0.4906 - val_loss: 1.2670 - val_accuracy: 0.5165\n",
            "Epoch 37/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.2499 - accuracy: 0.5041 - val_loss: 1.2585 - val_accuracy: 0.4789\n",
            "Epoch 38/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.2340 - accuracy: 0.5048 - val_loss: 1.2385 - val_accuracy: 0.4919\n",
            "Epoch 39/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.2132 - accuracy: 0.5172 - val_loss: 1.2163 - val_accuracy: 0.5296\n",
            "Epoch 40/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.1916 - accuracy: 0.5275 - val_loss: 1.1788 - val_accuracy: 0.5517\n",
            "Epoch 41/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.1730 - accuracy: 0.5342 - val_loss: 1.1627 - val_accuracy: 0.5322\n",
            "Epoch 42/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.1540 - accuracy: 0.5434 - val_loss: 1.1440 - val_accuracy: 0.5296\n",
            "Epoch 43/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.1381 - accuracy: 0.5495 - val_loss: 1.1378 - val_accuracy: 0.5184\n",
            "Epoch 44/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.1181 - accuracy: 0.5589 - val_loss: 1.1242 - val_accuracy: 0.5920\n",
            "Epoch 45/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.1000 - accuracy: 0.5716 - val_loss: 1.1115 - val_accuracy: 0.5261\n",
            "Epoch 46/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.0825 - accuracy: 0.5798 - val_loss: 1.0774 - val_accuracy: 0.6108\n",
            "Epoch 47/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.0662 - accuracy: 0.5924 - val_loss: 1.0506 - val_accuracy: 0.5877\n",
            "Epoch 48/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.0525 - accuracy: 0.5979 - val_loss: 1.0383 - val_accuracy: 0.5942\n",
            "Epoch 49/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.0272 - accuracy: 0.6135 - val_loss: 1.0126 - val_accuracy: 0.6182\n",
            "Epoch 50/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 1.0210 - accuracy: 0.6123 - val_loss: 0.9923 - val_accuracy: 0.6314\n",
            "Epoch 51/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.9927 - accuracy: 0.6266 - val_loss: 0.9782 - val_accuracy: 0.6579\n",
            "Epoch 52/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.9867 - accuracy: 0.6248 - val_loss: 0.9889 - val_accuracy: 0.6652\n",
            "Epoch 53/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.9652 - accuracy: 0.6362 - val_loss: 0.9574 - val_accuracy: 0.6255\n",
            "Epoch 54/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.9468 - accuracy: 0.6449 - val_loss: 0.9320 - val_accuracy: 0.6210\n",
            "Epoch 55/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.9337 - accuracy: 0.6498 - val_loss: 0.9630 - val_accuracy: 0.5895\n",
            "Epoch 56/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.9194 - accuracy: 0.6543 - val_loss: 1.0034 - val_accuracy: 0.5720\n",
            "Epoch 57/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.9039 - accuracy: 0.6667 - val_loss: 0.9089 - val_accuracy: 0.6326\n",
            "Epoch 58/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.8911 - accuracy: 0.6695 - val_loss: 0.9361 - val_accuracy: 0.6244\n",
            "Epoch 59/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.8789 - accuracy: 0.6774 - val_loss: 0.8847 - val_accuracy: 0.7004\n",
            "Epoch 60/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.8590 - accuracy: 0.6870 - val_loss: 0.9011 - val_accuracy: 0.7026\n",
            "Epoch 61/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.8564 - accuracy: 0.6920 - val_loss: 0.8491 - val_accuracy: 0.6489\n",
            "Epoch 62/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.8324 - accuracy: 0.7021 - val_loss: 0.8363 - val_accuracy: 0.7286\n",
            "Epoch 63/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.8269 - accuracy: 0.7052 - val_loss: 0.8012 - val_accuracy: 0.7276\n",
            "Epoch 64/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.8120 - accuracy: 0.7183 - val_loss: 0.8998 - val_accuracy: 0.6809\n",
            "Epoch 65/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.8048 - accuracy: 0.7196 - val_loss: 0.8352 - val_accuracy: 0.7103\n",
            "Epoch 66/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7930 - accuracy: 0.7263 - val_loss: 0.7694 - val_accuracy: 0.7404\n",
            "Epoch 67/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7814 - accuracy: 0.7340 - val_loss: 0.8207 - val_accuracy: 0.6969\n",
            "Epoch 68/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7811 - accuracy: 0.7312 - val_loss: 0.7827 - val_accuracy: 0.7246\n",
            "Epoch 69/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7638 - accuracy: 0.7452 - val_loss: 0.7583 - val_accuracy: 0.7612\n",
            "Epoch 70/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7515 - accuracy: 0.7534 - val_loss: 0.7530 - val_accuracy: 0.7574\n",
            "Epoch 71/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7454 - accuracy: 0.7548 - val_loss: 0.7489 - val_accuracy: 0.7744\n",
            "Epoch 72/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7303 - accuracy: 0.7651 - val_loss: 0.7274 - val_accuracy: 0.7783\n",
            "Epoch 73/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.7230 - accuracy: 0.7679 - val_loss: 0.7585 - val_accuracy: 0.7510\n",
            "Epoch 74/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.7119 - accuracy: 0.7758 - val_loss: 0.7403 - val_accuracy: 0.7421\n",
            "Epoch 75/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.7026 - accuracy: 0.7781 - val_loss: 0.6849 - val_accuracy: 0.7863\n",
            "Epoch 76/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6921 - accuracy: 0.7832 - val_loss: 0.7255 - val_accuracy: 0.7751\n",
            "Epoch 77/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6920 - accuracy: 0.7831 - val_loss: 0.7003 - val_accuracy: 0.7844\n",
            "Epoch 78/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6756 - accuracy: 0.7920 - val_loss: 0.6692 - val_accuracy: 0.7843\n",
            "Epoch 79/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6701 - accuracy: 0.7922 - val_loss: 0.6700 - val_accuracy: 0.7887\n",
            "Epoch 80/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6598 - accuracy: 0.7980 - val_loss: 0.6570 - val_accuracy: 0.8168\n",
            "Epoch 81/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6438 - accuracy: 0.8084 - val_loss: 0.6370 - val_accuracy: 0.8175\n",
            "Epoch 82/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.6417 - accuracy: 0.8053 - val_loss: 0.6175 - val_accuracy: 0.8264\n",
            "Epoch 83/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6240 - accuracy: 0.8155 - val_loss: 0.6639 - val_accuracy: 0.7918\n",
            "Epoch 84/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6234 - accuracy: 0.8137 - val_loss: 0.6353 - val_accuracy: 0.8053\n",
            "Epoch 85/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.6168 - accuracy: 0.8153 - val_loss: 0.6034 - val_accuracy: 0.8333\n",
            "Epoch 86/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5960 - accuracy: 0.8269 - val_loss: 0.6114 - val_accuracy: 0.8145\n",
            "Epoch 87/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5896 - accuracy: 0.8288 - val_loss: 0.5812 - val_accuracy: 0.8465\n",
            "Epoch 88/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5854 - accuracy: 0.8288 - val_loss: 0.5868 - val_accuracy: 0.8167\n",
            "Epoch 89/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5713 - accuracy: 0.8360 - val_loss: 0.5755 - val_accuracy: 0.8232\n",
            "Epoch 90/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5676 - accuracy: 0.8362 - val_loss: 0.6055 - val_accuracy: 0.8021\n",
            "Epoch 91/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.5624 - accuracy: 0.8369 - val_loss: 0.5521 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5544 - accuracy: 0.8402 - val_loss: 0.5419 - val_accuracy: 0.8638\n",
            "Epoch 93/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5367 - accuracy: 0.8513 - val_loss: 0.5408 - val_accuracy: 0.8571\n",
            "Epoch 94/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5397 - accuracy: 0.8449 - val_loss: 0.5271 - val_accuracy: 0.8722\n",
            "Epoch 95/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5322 - accuracy: 0.8475 - val_loss: 0.5649 - val_accuracy: 0.8253\n",
            "Epoch 96/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5144 - accuracy: 0.8579 - val_loss: 0.5586 - val_accuracy: 0.8240\n",
            "Epoch 97/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5157 - accuracy: 0.8550 - val_loss: 0.5816 - val_accuracy: 0.8107\n",
            "Epoch 98/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.5061 - accuracy: 0.8590 - val_loss: 0.5022 - val_accuracy: 0.8729\n",
            "Epoch 99/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.4999 - accuracy: 0.8593 - val_loss: 0.5352 - val_accuracy: 0.8404\n",
            "Epoch 100/100\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.4988 - accuracy: 0.8553 - val_loss: 0.5203 - val_accuracy: 0.8523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CzHcmCKo6bN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "85834021-d1e7-460e-9959-6a254eda1b4d"
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        "score_list=[]\n",
        "history_list=[]\n",
        " \n",
        " score = model.evaluate(x_test, y_test, verbose=0)\n",
        " score_list.append(score)\n",
        " history_list.append(history)\n",
        " print(score)\n",
        " print(history)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5203434824943542, 0.8522999882698059]\n",
            "<tensorflow.python.keras.callbacks.History object at 0x7f0b658c0a90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX3GuQV7d89l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwYlz6ONd_6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (model.layers[0].get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhMm8zUJeDRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習したモデルにXを与えた際の結果\n",
        "Y2 = model.predict_classes(X, batch_size=1)\n",
        "\n",
        "print (Y2)\n",
        "print (Y==Y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnH1ZF5TnJhG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5-0GARueGL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習過程(loss)の可視化\n",
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "#plt.ylim(0.5,1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ6-3_sAeKN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "1c03707a-48db-4199-ee83-32c2a6991bfa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "score_list=[]\n",
        "history_list=[]\n",
        "\n",
        "for i in range(10):\n",
        " x_train, x_valid, y_train, y_valid = train_test_split(X,Y, test_size=0.1, shuffle=True)\n",
        "\n",
        " y_train = np_utils.to_categorical(y_train)\n",
        " y_valid2 = np_utils.to_categorical(y_valid)\n",
        "\n",
        " model = Sequential()\n",
        " model.add(Dense(units=8, input_dim=4))\n",
        " model.add(Activation('sigmoid'))\n",
        " model.add(Dense(3,activation='softmax'))\n",
        "\n",
        " model.compile(loss='categorical_crossentropy',optimizer=SGD(lr=0.1),metrics=['accuracy'])\n",
        "\n",
        " history = model.fit(x_train, y_train, epochs=100, batch_size=1, validation_data=(x_valid,y_valid2))\n",
        "\n",
        " score = model.evaluate(x_valid, y_valid2, verbose=0)\n",
        " score_list.append(score)\n",
        " history_list.append(history)\n",
        " print(score)\n",
        " print(history)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-146ee84e6e2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m  \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m  \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knuXLF-neNIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  plt.plot(history_list[i].history['loss'])\n",
        "  plt.plot(history_list[i].history['val_accuracy'])\n",
        "\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}