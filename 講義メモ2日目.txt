kerasではステップ関数は使えないのでシグモイド関数で実行
論理積→線形分離

形式ニューロン
2次元のデータに対して1本の線で分離可能
形式ニューロンではXORは計算できない→2個のデータに対し2分離するには
2本の線が必要→線形分離不可能

→そこで階層を増やすことを提案
形式ニューロンと単純パーセプトロン
シグモイド関数を使ったモデル→ロジスティック回帰

収束定理
有限回(一定の回数)

循環定理
n-1で解くことが不可能(線形分離不可能)

誤差逆伝播法★
学習を効率的に行う方法の1つ


尤度関数とエントロピー
p(Y=1|x)=f(wTx+b)
答えを1か0とすると2つの式をまとめられる

L(w,b)=記号p(Y=z_n|x_n)
入力Nがに対して確率を全て掛け合わせる

掛け算はコストがかかる→エネルギー最小化問題
最小にするためにマイナスをつけている
-Σ{z_n log y_n + (1-z_n)log(1-y_n)}
E:誤差関数あるいは損失関数


確率密度関数:確率変数xがa以上b以下となる確率を求めることが可能な関数
累積分布関数:ある基準点z以下の面積

正規化:なん標準偏差分かをもとめる

累積分布を計算にはニューラルネットワークの求まった解のもっともらしさを
正規分布にあてはめて計算する必要がある
→勾配計算が大変なのでシグモイド関数で近似

多クラスパーセプトロン→多クラスロジスティック回帰
誤差を計算する際も多クラスに対応させる必要がある
→ソフトマックス関数
xに対してそれぞれのe^xを足し合わせたものを分母にe~iを1個づつやる
全体が1になるように
kクラスの時のそれぞれの答えにeを加えて全体を分母にそれぞれの
割合を求める



4回
機械学習→最適化問題
誤差をコスト関数としてその最小値を求める

微分→変化の傾向を見る

最小値となるポイント→微分した式が0になる


ニューラルネットワークの評価
yが複数の場合のコスト関数は回帰分析で求める
誤差→答え-予測値
相殺を防ぐために2乗誤差Ckを計算する
総和C=C1+C2+C3+....→誤差の合計が最小になるようにする
CやC1の式は合成関数→合成関数の偏微分をおこなう
誤差を偏微分し,0になるような偏回帰係数や切片を求める

5回
データ数に適したモデルの構造やパラメータの制限が存在する
偏微分をやや楽にする勾配降下法
誤差を求めて誤差の勾配を求めてコスト関数に対して偏微分

ユニットの誤差デルタの導入
勾配法よりもさらに楽にしたもの→誤差逆伝播法
パラメータの誤差→デルタ
デルタとaを掛け合わせたものがCでのwの偏微分と等しくなる
誤差Cは合成関数に分けることができそれらをδに置き換えることが
できる→スライド通り

誤差逆伝番法　出力層の誤差

δは出力層から求めることができる

デルタの値を計算し後ろの層から前の層のδを決める→誤差伝播法





